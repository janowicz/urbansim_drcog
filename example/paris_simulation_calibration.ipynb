{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "seconds_start = time.time()\n",
      "print seconds_start\n",
      "import numpy as np, pandas as pd, os, statsmodels.api as sm\n",
      "import synthicity.urbansim.interaction as interaction\n",
      "from synthicity.utils import misc\n",
      "import dataset, copy, math\n",
      "np.random.seed(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1385086503.72\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resunit_targets = np.array([.0077,.0661,.0327,.0415,.0397,.0086,.0272,.0075,.0490,.0032,.2033,.5137])\n",
      "hh_targets = np.array([.0120,.0792,.0377,.0506,.0453,.0118,.0437,.0091,.0524,.0040,.1156,.5386])\n",
      "emp_targets = np.array([.0064,.0304,.0225,.0374,.0840,.0001,.0616,.0701,.0550,.0086,.2074,.4169])\n",
      "zgpgroup_id = np.array([21, 22, 23, 24, 25, 26, 47, 48, 49, 50, 75, 99])\n",
      "targets = pd.DataFrame({'zgpgroup_id':zgpgroup_id,'resunit_target':resunit_targets,'hh_target':hh_targets,'emp_target':emp_targets,'nr_target':emp_targets})\n",
      "targets = targets.set_index('zgpgroup_id')\n",
      "print targets\n",
      "first_year = 2000\n",
      "last_year = 2010\n",
      "iterations = 100\n",
      "delta = .2\n",
      "hh_submodels = ['hh_location_1', 'hh_location_2', 'hh_location_3', 'hh_location_4']\n",
      "emp_submodels = ['emp_location_1', 'emp_location_2', 'emp_location_3', 'emp_location_4', 'emp_location_5', 'emp_location_6', 'emp_location_7', 'emp_location_8', 'emp_location_9', 'emp_location_10', 'emp_location_11']\n",
      "ru_submodels = ['dp_location_1', 'dp_location_2', 'dp_location_3', 'dp_location_4']\n",
      "nr_submodels = ['dp_location_6',]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             emp_target  hh_target  nr_target  resunit_target\n",
        "zgpgroup_id                                                  \n",
        "21               0.0064     0.0120     0.0064          0.0077\n",
        "22               0.0304     0.0792     0.0304          0.0661\n",
        "23               0.0225     0.0377     0.0225          0.0327\n",
        "24               0.0374     0.0506     0.0374          0.0415\n",
        "25               0.0840     0.0453     0.0840          0.0397\n",
        "26               0.0001     0.0118     0.0001          0.0086\n",
        "47               0.0616     0.0437     0.0616          0.0272\n",
        "48               0.0701     0.0091     0.0701          0.0075\n",
        "49               0.0550     0.0524     0.0550          0.0490\n",
        "50               0.0086     0.0040     0.0086          0.0032\n",
        "75               0.2074     0.1156     0.2074          0.2033\n",
        "99               0.4169     0.5386     0.4169          0.5137\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for it in range(iterations):\n",
      "    print 'Calibration iteration ' + str(it)\n",
      "    summary = {'employment':[],'households':[],'non_residential_sqft':[],'residential_units':[],'price':[]}\n",
      "    dset = dataset.PARISDataset(os.path.join(misc.data_dir(),'paris.h5'))\n",
      "    \n",
      "    for sim_year in range(first_year,last_year+1):\n",
      "        print 'Simulating year ' + str(sim_year)\n",
      "        \n",
      "    #####Variable calculations\n",
      "        b = dset.fetch('buildings')\n",
      "        if sim_year==first_year:\n",
      "            b = pd.merge(b,dset.store.building_sqft_per_job,left_index=True,right_index=True)\n",
      "        b=b[['building_type_id','non_residential_sqft','non_residential_sqft_capacity','price','residential_units','residential_units_capacity','zone_id','building_sqft_per_job','estim_index']]\n",
      "    \n",
      "        b['ln_average_res_price1'] = b.price[b.building_type_id==1].apply(np.log1p)\n",
      "        b['ln_average_res_price2'] = b.price[b.building_type_id==2].apply(np.log1p)\n",
      "        b['ln_average_res_price3'] = b.price[b.building_type_id==3].apply(np.log1p)\n",
      "        b['ln_average_res_price4'] = b.price[b.building_type_id==4].apply(np.log1p)\n",
      "        b['ln_average_nonres_price6'] = b.price[b.building_type_id==6].apply(np.log1p)\n",
      "        b['price1'] = b.price[b.building_type_id==1]\n",
      "        b['price2'] = b.price[b.building_type_id==2]\n",
      "        b['price3'] = b.price[b.building_type_id==3]\n",
      "        b['price4'] = b.price[b.building_type_id==4]\n",
      "        b['ln_residential_units_owner_house'] = b.residential_units[b.building_type_id==1].apply(np.log1p)\n",
      "        b['ln_residential_units_owner_flat'] = b.residential_units[b.building_type_id==2].apply(np.log1p)\n",
      "        b['ln_residential_units_renter_house'] = b.residential_units[b.building_type_id==3].apply(np.log1p)\n",
      "        b['ln_residential_units_renter_flat'] = b.residential_units[b.building_type_id==4].apply(np.log1p)\n",
      "        #household\n",
      "        hh = dset.fetch('households')\n",
      "        hh['zone_id'] = b.zone_id[hh.building_id].values\n",
      "        if sim_year==first_year:\n",
      "            hh['foreign'] = (hh.race_id==1).astype('int32')\n",
      "            hh['income_cat'] = 1*(hh.lincomepc<=9.9) + 2*(hh.lincomepc>9.9)*(hh.lincomepc<=10.3) + 3*(hh.lincomepc>10.3)\n",
      "            hh['high_inc'] = (hh.income_cat==3).astype('int32')\n",
      "            hh['mid_inc'] = (hh.income_cat==2).astype('int32')\n",
      "            hh['low_inc'] = (hh.income_cat==1).astype('int32')\n",
      "            hh['age_cat'] = 1*(hh.age_of_head<=35) + 2*(hh.age_of_head>35)*(hh.age_of_head<=60) + 3*(hh.age_of_head>60)\n",
      "            hh['btype_tenure'] = 1*np.in1d(hh.hh_type,[1,5,9,13,17,21]) + 2*np.in1d(hh.hh_type,[2,6,10,14,18,22]) + 3*np.in1d(hh.hh_type,[3,7,11,15,19,23]) + 4*np.in1d(hh.hh_type,[4,8,12,16,20,24])\n",
      "            hh['hhsize3plus'] = (hh.persons>2).astype('int32')\n",
      "            hh['hhsize2'] = (hh.persons==2).astype('int32')\n",
      "            hh['young'] = (hh.age_cat==1).astype('int32')\n",
      "            hh['middle_age'] = (hh.age_cat==2).astype('int32')\n",
      "            hh['old'] = (hh.age_cat==3).astype('int32')\n",
      "            hh['with_child'] = (hh.children>0).astype('int32')\n",
      "            hh['with_car'] = (hh.cars>0).astype('int32')\n",
      "            hh['previous_county'] = 1+1*(hh.previous_dpt==75)+2*(np.in1d(hh.previous_dpt,[92,93,94]))\n",
      "        #establishment\n",
      "        e = dset.fetch('establishments')\n",
      "        e['zone_id'] = b.zone_id[e.building_id].values\n",
      "        if sim_year==first_year:\n",
      "            e['more_than_10_employees'] = (e.employees>10).astype('int32')\n",
      "            e['less_than_10_employees'] = (e.employees<10).astype('int32')\n",
      "        e1998 = dset.fetch('establishments1998')\n",
      "        if sim_year==first_year:\n",
      "            e1998['zone_id'] = b.zone_id[e1998.building_id].values\n",
      "        \n",
      "        #zone\n",
      "        z = dset.fetch('zones')\n",
      "        z['mean_household_size'] = hh.groupby('zone_id').persons.mean()\n",
      "        z['mean_age_of_head'] = hh.groupby('zone_id').age_of_head.mean()\n",
      "        zonal_household_totals = hh.groupby('zone_id').building_id.count()\n",
      "        z['total_households'] = hh.groupby('zone_id').building_id.count()\n",
      "        z['total_residential_units'] = b.groupby('zone_id').residential_units.sum()\n",
      "        z['total_nonresidential_sqft'] = b.groupby('zone_id').non_residential_sqft.sum()\n",
      "        z['percent_foreigners'] = hh.groupby('zone_id').foreign.sum()*100.0/(zonal_household_totals)\n",
      "        z['percent_low_income'] = hh[hh.income_cat==1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_mid_income'] = hh[hh.income_cat==2].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_high_income'] = hh[hh.income_cat==3].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hhsize3plus'] = hh[hh.hhsize3plus==1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hhsize2'] = hh[hh.hhsize2==1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_young'] = hh[hh.age_cat==1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_middle_age'] = hh[hh.age_cat==2].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_old'] = hh[hh.age_cat==3].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_with_child'] = hh[hh.children>0].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hh_zero_worker'] = hh[hh.workers==0].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hh_one_worker'] = hh[hh.workers==1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hh_two_worker'] = hh[hh.workers==2].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hh_twoplus_workers'] = hh[hh.workers>1].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['percent_hh_threeplus_workers'] = hh[hh.workers>2].groupby('zone_id').building_id.count()*100.0/(zonal_household_totals)\n",
      "        z['population_density'] = z.crrdenspop\n",
      "        travel_data = dset.store.travel_data_baseline[dset.store.travel_data_baseline.year==sim_year]\n",
      "        z['tco'] = travel_data.tco\n",
      "        z['vpo'] = travel_data.vpo\n",
      "        z['tcd'] = travel_data.tcd\n",
      "        z['vpd'] = travel_data.vpd\n",
      "        if sim_year==first_year:\n",
      "            z['in_paris'] = (z.dept==75).astype('int32')\n",
      "            z['in_biotech'] = (z.zgp_id==22).astype('int32')\n",
      "            z['in_clichy_montfermeil'] = (z.zgp_id==21).astype('int32')\n",
      "            z['in_confluence'] = (z.zgp_id==23).astype('int32')\n",
      "            z['in_descartes'] = (z.zgp_id==24).astype('int32')\n",
      "            z['in_le_bourget'] = (z.zgp_id==26).astype('int32')\n",
      "            z['in_paris_pole'] = ((z.zgp_id>=27)*(z.zgp_id<=46)).astype('int32')\n",
      "            z['in_pleyel'] = (z.zgp_id==47).astype('int32')\n",
      "            z['in_roissy'] = (z.zgp_id==48).astype('int32')\n",
      "            z['in_saclay'] = (z.zgp_id==49).astype('int32')\n",
      "            z['in_val_de_france_gonesse'] = (z.zgp_id==50).astype('int32')\n",
      "            z['in_la_defense'] = np.in1d(z.insee,[92062,92026,92050]).astype('int32') \n",
      "            z['in_new_town'] = (z.cvilnouvel>0).astype('int32')  \n",
      "            z['in_paris_suburbs'] = np.in1d(z.dept_id,[92,93,94]).astype('int32') \n",
      "            z['distance_to_arterial'] = z.cdistart/1000\n",
      "            z['distance_to_highway'] = z.cdisthwy/1000\n",
      "            z['ln_land_area'] = z.careakm2.apply(np.log1p)\n",
      "            z['cloactpotst'] = z.cnoactpotst.apply(np.log1p)\n",
      "            z['tax_on_professionals'] = z.taxpro\n",
      "            z['percent_education_level1'] = z.ctpniv1\n",
      "            z['percent_education_level2'] = z.ctpniv2\n",
      "            z['percent_education_level3'] = z.ctpniv3\n",
      "            z['percent_education_level4'] = z.ctpniv4\n",
      "            z['zgpgroup_id'] = 99*(z.zgp_id<21) + 21*(z.zgp_id==21) + 22*(z.zgp_id==22)+23*(z.zgp_id==23)+24*(z.zgp_id==24)+25*(z.zgp_id==25)+26*(z.zgp_id==26) +75*(z.zgp_id>26)*(z.zgp_id<47) + 47*(z.zgp_id==47) + 48*(z.zgp_id==48) + 49*(z.zgp_id==49) + 50*(z.zgp_id==50)\n",
      "            z['zgpgroup21'] = (z.zgpgroup_id ==21).astype('int32')\n",
      "            z['zgpgroup22'] = (z.zgpgroup_id ==22).astype('int32')\n",
      "            z['zgpgroup23'] = (z.zgpgroup_id ==23).astype('int32')\n",
      "            z['zgpgroup24'] = (z.zgpgroup_id ==24).astype('int32')\n",
      "            z['zgpgroup25'] = (z.zgpgroup_id ==25).astype('int32')\n",
      "            z['zgpgroup26'] = (z.zgpgroup_id ==26).astype('int32')\n",
      "            z['zgpgroup47'] = (z.zgpgroup_id ==47).astype('int32')\n",
      "            z['zgpgroup48'] = (z.zgpgroup_id ==48).astype('int32')\n",
      "            z['zgpgroup49'] = (z.zgpgroup_id ==49).astype('int32')\n",
      "            z['zgpgroup50'] = (z.zgpgroup_id ==50).astype('int32')\n",
      "            z['zgpgroup75'] = (z.zgpgroup_id ==75).astype('int32')\n",
      "            z['zgpgroup99'] = (z.zgpgroup_id ==99).astype('int32')\n",
      "        z['total_employment'] = e.groupby('zone_id').employees.sum()\n",
      "        z['total_employment_prev_yr'] = e1998.groupby('zone_id').employees.sum()\n",
      "        z['employment_density'] = z.total_employment/z.careakm2\n",
      "        z['prev_yr_empdensity_sector2'] = e1998[e1998.sector_id==2].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector3'] = e1998[e1998.sector_id==3].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector4'] = e1998[e1998.sector_id==4].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector5'] = e1998[e1998.sector_id==5].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector6'] = e1998[e1998.sector_id==6].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector7'] = e1998[e1998.sector_id==7].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector8'] = e1998[e1998.sector_id==8].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector9'] = e1998[e1998.sector_id==9].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector10'] = e1998[e1998.sector_id==10].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        z['prev_yr_empdensity_sector11'] = e1998[e1998.sector_id==11].groupby('zone_id').employees.sum()*1000/z.cnoactpotst\n",
      "        \n",
      "        #Reset for lag variables as simulation progresses, but before establishments get updated this year\n",
      "        dset.d['establishments1998'] = e.copy()\n",
      "    \n",
      "        #merge buildings with zones\n",
      "        dset.d['buildings'] = pd.merge(b,z,left_on='zone_id',right_index=True)\n",
      "        \n",
      "        #Record 1999 values for temporal comparison\n",
      "        if sim_year==first_year:\n",
      "            summary['employment'].append(e[e.building_id>0].employees.sum())\n",
      "            summary['households'].append(len(hh[hh.building_id>0].building_id))\n",
      "            summary['non_residential_sqft'].append(b.non_residential_sqft.sum())\n",
      "            summary['residential_units'].append(b.residential_units.sum())\n",
      "            summary['price'].append(b.price.mean())\n",
      "            base_emp = z.groupby('zgpgroup_id').total_employment.sum()\n",
      "            base_hh = z.groupby('zgpgroup_id').total_households.sum()\n",
      "            base_ru = z.groupby('zgpgroup_id').total_residential_units.sum()\n",
      "            base_nr = z.groupby('zgpgroup_id').total_nonresidential_sqft.sum()\n",
      "            \n",
      "        \n",
      "        ##Estimate REPM instead of loading from CSV because it is so fast\n",
      "        if sim_year==first_year:\n",
      "            buildings = dset.fetch('buildings')\n",
      "            buildings = buildings[buildings.estim_index==1]\n",
      "            output_csv, output_title, coeff_name, output_varname = [\"paris-coeff-hedonic-%s.csv\",\"PARIS HEDONIC MODEL (%s)\",\"price_%s\",\"price\"]\n",
      "            ind_vars1 = ['in_paris_suburbs','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                         'population_density','tco',]\n",
      "            ind_vars2 = ['in_paris','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                         'population_density','tco',]\n",
      "            ind_vars3 = ['in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old',\n",
      "                         'population_density','tco',]\n",
      "            ind_vars4 = ['in_paris','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                         'population_density','tco',]\n",
      "            ind_vars6 = ['in_paris','in_la_defense','in_new_town','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','bati',\n",
      "                         'employment_density','tax_on_professionals','tco',]\n",
      "            segments = buildings.groupby('building_type_id')\n",
      "            for name, segment in segments:\n",
      "                if name == 1:\n",
      "                    indvars = ind_vars1\n",
      "                if name == 2:\n",
      "                    indvars = ind_vars2\n",
      "                if name == 3:\n",
      "                    indvars = ind_vars3\n",
      "                if name == 4:\n",
      "                    indvars = ind_vars4\n",
      "                if name == 6:\n",
      "                    indvars = ind_vars6\n",
      "                est_data = pd.DataFrame(index=segment.index)\n",
      "                for varname in indvars:\n",
      "                    est_data[varname] = segment[varname]\n",
      "                est_data = est_data.fillna(0)\n",
      "                est_data = sm.add_constant(est_data,prepend=False)\n",
      "                tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name\n",
      "                #print tmp_coeffname\n",
      "                depvar = segment['price'].apply(np.log)\n",
      "                print \"Estimating hedonic for %s with %d observations\" % (name,len(segment.index))\n",
      "                #print est_data.describe()\n",
      "                model = sm.OLS(depvar,est_data)\n",
      "                results = model.fit()\n",
      "                #print results.summary()\n",
      "                tmp_outcsv = output_csv%name\n",
      "                tmp_outtitle = output_title%name\n",
      "                misc.resultstocsv((results.rsquared,results.rsquared_adj),est_data.columns,\n",
      "                                    zip(results.params,results.bse,results.tvalues),tmp_outcsv,hedonic=1,\n",
      "                                    tblname=output_title)\n",
      "                dset.store_coeff(tmp_coeffname,results.params.values,results.params.index)\n",
      "                \n",
      "            ##Load location choice model coefficients from csv or hdf5\n",
      "            output_dir = os.path.join(os.environ['DATA_HOME'],'output')\n",
      "            output_dir = os.path.join(output_dir, 'for_runs')\n",
      "            coeff_store_path = os.path.join(output_dir,'coeffs.h5')\n",
      "            if os.path.exists(coeff_store_path):\n",
      "                coeff_store = pd.HDFStore(coeff_store_path)\n",
      "                dset.coeffs = coeff_store.coeffs.copy()\n",
      "                coeff_store.close()\n",
      "            else:\n",
      "                variable_names = np.array(['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99'])\n",
      "                coefficient = np.array([.0001]*12)\n",
      "                stderr = np.array([.0]*12)\n",
      "                tscore = np.array([.0]*12)\n",
      "                significance = np.array(['calib']*12)\n",
      "                to_append = pd.DataFrame({'Variables':variable_names,'Coefficient':coefficient,'Stderr':stderr,'T-score':tscore,'Significance':significance})\n",
      "                \n",
      "                output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-hlcm-%s.csv\",\"PARIS HOUSEHOLD LOCATION CHOICE MODELS (%s)\",\"hh_location_%s\",\"household_building_ids\")\n",
      "                segments = [1,2,3,4]\n",
      "                for name in segments:\n",
      "                    name = str(name)\n",
      "                    tmp_outcsv, tmp_coeffname = output_csv%name, coeff_name%name\n",
      "                    csv_path = os.path.join(output_dir,tmp_outcsv)\n",
      "                    df = pd.read_csv(csv_path,skiprows=5)\n",
      "                    df = pd.concat([df,to_append],ignore_index=True)\n",
      "                    coeffs = df.Coefficient\n",
      "                    fnames = df.Variables.str.replace(' ','_')\n",
      "                    #print tmp_coeffname\n",
      "                    colname1 = (tmp_coeffname,'coeffs')\n",
      "                    colname2 = (tmp_coeffname,'fnames')\n",
      "                    d = {colname1:coeffs, colname2:fnames}\n",
      "                    d[colname2] = fnames\n",
      "                    dset.coeffs = pd.concat([dset.coeffs,pd.DataFrame(d)],axis=1)\n",
      "                output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-elcm-%s.csv\",\"PARIS EMPLOYMENT LOCATION CHOICE MODELS (%s)\",\"emp_location_%s\",\"establishment_building_ids\")\n",
      "                segments = [1,2,3,4,5,6,7,8,9,10,11]\n",
      "                for name in segments:\n",
      "                    name = str(name)\n",
      "                    tmp_outcsv, tmp_coeffname = output_csv%name, coeff_name%name\n",
      "                    csv_path = os.path.join(output_dir,tmp_outcsv)\n",
      "                    df = pd.read_csv(csv_path,skiprows=5)\n",
      "                    df = pd.concat([df,to_append],ignore_index=True)\n",
      "                    coeffs = df.Coefficient\n",
      "                    fnames = df.Variables.str.replace(' ','_')\n",
      "                    #print tmp_coeffname\n",
      "                    colname1 = (tmp_coeffname,'coeffs')\n",
      "                    colname2 = (tmp_coeffname,'fnames')\n",
      "                    d = {colname1:coeffs, colname2:fnames}\n",
      "                    d[colname2] = fnames\n",
      "                    dset.coeffs = pd.concat([dset.coeffs,pd.DataFrame(d)],axis=1)\n",
      "                output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-dplcm-%s.csv\",\"PARIS DEVPROJECT LOCATION CHOICE MODELS (%s)\",\"dp_location_%s\",\"devproject_building_ids\")\n",
      "                segments = [1,2,3,4,6]\n",
      "                for name in segments:\n",
      "                    name = str(name)\n",
      "                    tmp_outcsv, tmp_coeffname = output_csv%name, coeff_name%name\n",
      "                    csv_path = os.path.join(output_dir,tmp_outcsv)\n",
      "                    df = pd.read_csv(csv_path,skiprows=5)\n",
      "                    df = pd.concat([df,to_append],ignore_index=True)\n",
      "                    coeffs = df.Coefficient\n",
      "                    fnames = df.Variables.str.replace(' ','_')\n",
      "                    #print tmp_coeffname\n",
      "                    colname1 = (tmp_coeffname,'coeffs')\n",
      "                    colname2 = (tmp_coeffname,'fnames')\n",
      "                    d = {colname1:coeffs, colname2:fnames}\n",
      "                    d[colname2] = fnames\n",
      "                    dset.coeffs = pd.concat([dset.coeffs,pd.DataFrame(d)],axis=1)\n",
      "                \n",
      "                \n",
      "    ############     ELCM\n",
      "        output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-elcm-%s.csv\",\"PARIS EMPLOYMENT LOCATION CHOICE MODELS (%s)\",\"emp_location_%s\",\"establishment_building_ids\")\n",
      "        dset.establishments['home_based_status']=0\n",
      "        new_jobs = {\"table\": \"dset.establishments\",\"writetotmp\": \"establishments\",\"model\": \"transitionmodel\",\"first_year\": 1999,\"control_totals\": \"dset.annual_employment_control_totals\",\n",
      "                    \"geography_field\": \"building_id\",\"amount_field\": \"number_of_jobs\",\"size_field\":\"employees\"}\n",
      "        import synthicity.urbansim.transitionmodel as transitionmodel\n",
      "        transitionmodel.simulate(dset,new_jobs,year=sim_year,show=True)\n",
      "        year = sim_year\n",
      "        choosers = dset.fetch('establishments')\n",
      "        depvar = 'building_id'\n",
      "    #     rate_table = dset.annual_job_relocation_rates\n",
      "    #     rate_table = rate_table*.1\n",
      "    #     rate_field = \"job_relocation_probability\"\n",
      "    #     movers = dset.relocation_rates(choosers,rate_table,rate_field)\n",
      "    #     choosers[depvar].ix[movers] = -1\n",
      "        movers = choosers[choosers[depvar]==-1]\n",
      "        print \"Total new agents and movers = %d\" % len(movers.index)\n",
      "        alternatives = dset.buildings[(dset.buildings.non_residential_sqft>0)*(dset.buildings.building_type_id==6)]\n",
      "        alternatives['job_spaces'] = alternatives.non_residential_sqft/alternatives.building_sqft_per_job\n",
      "        empty_units = alternatives.job_spaces.sub(choosers.groupby('building_id').employees.sum(),fill_value=0).astype('int')\n",
      "        alts = alternatives.ix[empty_units.index]\n",
      "        alts[\"supply\"] = empty_units\n",
      "        lotterychoices = True\n",
      "        pdf = pd.DataFrame(index=alts.index)\n",
      "        segments = movers.groupby(['sector_id','more_than_10_employees'])\n",
      "        \n",
      "        ind_vars1=['in_paris','distance_to_highway','total_employment_prev_yr','ctrain9','prev_yr_empdensity_sector2','prev_yr_empdensity_sector3','prev_yr_empdensity_sector4','prev_yr_empdensity_sector5',\n",
      "                   'prev_yr_empdensity_sector8','population_density','tcd','vpd'] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars2=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_more_than_10_employees','in_confluence','prev_yr_empdensity_sector2','prev_yr_empdensity_sector3',\n",
      "                   'prev_yr_empdensity_sector4','prev_yr_empdensity_sector5','prev_yr_empdensity_sector8','population_density','tcd','percent_education_level1','percent_education_level2',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars3=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_average_nonres_price6','in_confluence','csubway9','ctrain9',\n",
      "                   'prev_yr_empdensity_sector2','prev_yr_empdensity_sector3','prev_yr_empdensity_sector4','population_density','tcd','vpd','percent_education_level1','percent_education_level2',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse'\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars4=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_more_than_10_employees','in_confluence','csubway9','percent_high_income','percent_low_income',\n",
      "                   'prev_yr_empdensity_sector2','prev_yr_empdensity_sector3','prev_yr_empdensity_sector4','prev_yr_empdensity_sector6','prev_yr_empdensity_sector9','population_density','tcd','vpd',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars5=['in_biotech','in_la_defense','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_more_than_10_employees','in_confluence','csubway9',\n",
      "                   'prev_yr_empdensity_sector2','prev_yr_empdensity_sector3','prev_yr_empdensity_sector5','population_density',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars6=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_less_than_10_employees','in_confluence','csubway9','ctrain9','percent_high_income',\n",
      "                   'prev_yr_empdensity_sector4','prev_yr_empdensity_sector6','prev_yr_empdensity_sector7','population_density','vpd','percent_education_level3','percent_education_level4',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ]  + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars7=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','in_confluence','csubway9','ctrain9','percent_high_income',\n",
      "                   'prev_yr_empdensity_sector3','prev_yr_empdensity_sector6','prev_yr_empdensity_sector7','prev_yr_empdensity_sector8','prev_yr_empdensity_sector9','population_density','tcd','vpd','percent_education_level3','percent_education_level4',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars8=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_less_than_10_employees','in_confluence','csubway9','ctrain9','percent_high_income','percent_low_income','percent_hh_zero_worker',\n",
      "                   'prev_yr_empdensity_sector2','prev_yr_empdensity_sector7','prev_yr_empdensity_sector8','tax_on_professionals','population_density','vpd','percent_education_level3','percent_education_level4',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars9=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_less_than_10_employees','in_confluence','csubway9','percent_old','percent_young','percent_high_income','percent_low_income','czfu',\n",
      "                   'percent_hh_zero_worker','percent_with_child','prev_yr_empdensity_sector9','population_density','vpd','tcd',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse'\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars10=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_average_nonres_price6','in_confluence','ctrain9','percent_old','percent_young','percent_high_income','percent_low_income',\n",
      "                   'percent_with_child','prev_yr_empdensity_sector5','prev_yr_empdensity_sector6','prev_yr_empdensity_sector10','prev_yr_empdensity_sector11','population_density','vpd',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse'\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars11=['in_biotech','in_la_defense','in_new_town','total_employment_prev_yr','in_clichy_montfermeil','cloactpotst','ln_nonres_price_x_more_than_10_employees','in_confluence',\n",
      "                   'prev_yr_empdensity_sector9','prev_yr_empdensity_sector10','prev_yr_empdensity_sector11','vpd',\n",
      "                   'in_descartes','in_le_bourget','in_paris_pole','in_pleyel','in_roissy','in_saclay','in_val_de_france_gonesse'\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        \n",
      "        for name, segment in segments:\n",
      "            if name[0] == 1:\n",
      "                ind_vars = ind_vars1 \n",
      "            if name[0] == 2:\n",
      "                ind_vars = ind_vars2\n",
      "            if name[0] == 3:\n",
      "                ind_vars = ind_vars3\n",
      "            if name[0] == 4:\n",
      "                ind_vars = ind_vars4\n",
      "            if name[0] == 5:\n",
      "                ind_vars = ind_vars5\n",
      "            if name[0] == 6:\n",
      "                ind_vars = ind_vars6\n",
      "            if name[0] == 7:\n",
      "                ind_vars = ind_vars7\n",
      "            if name[0] == 8:\n",
      "                ind_vars = ind_vars8\n",
      "            if name[0] == 9:\n",
      "                ind_vars = ind_vars9\n",
      "            if name[0] == 10:\n",
      "                ind_vars = ind_vars10\n",
      "            if name[0] == 11:\n",
      "                ind_vars = ind_vars11\n",
      "             \n",
      "            segment = segment.head(1)\n",
      "            name_coeff= str(name[0])\n",
      "            name = str(name)\n",
      "            tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name_coeff\n",
      "            SAMPLE_SIZE = alts.index.size \n",
      "            numchoosers = segment.shape[0]\n",
      "            numalts = alts.shape[0]\n",
      "            sample = np.tile(alts.index.values,numchoosers)\n",
      "            alts_sample = alts #sample#alternatives\n",
      "            alts_sample['join_index'] = np.repeat(segment.index,SAMPLE_SIZE)\n",
      "            alts_sample = pd.merge(alts_sample,segment,left_on='join_index',right_index=True,suffixes=('','_r'))\n",
      "            chosen = np.zeros((numchoosers,SAMPLE_SIZE))\n",
      "            chosen[:,0] = 1\n",
      "            sample, alternative_sample, est_params = sample, alts_sample, ('mnl',chosen)\n",
      "            alternative_sample['ln_nonres_price_x_more_than_10_employees'] = (alternative_sample.ln_average_nonres_price6*alternative_sample.more_than_10_employees)\n",
      "            alternative_sample['ln_nonres_price_x_less_than_10_employees'] = (alternative_sample.ln_average_nonres_price6*alternative_sample.less_than_10_employees)\n",
      "            est_data = pd.DataFrame(index=alternative_sample.index)\n",
      "            for varname in ind_vars:\n",
      "                est_data[varname] = alternative_sample[varname]\n",
      "            est_data = est_data.fillna(0)\n",
      "            data = est_data\n",
      "            data = data.as_matrix()\n",
      "            coeff = dset.load_coeff(tmp_coeffname)\n",
      "            probs = interaction.mnl_simulate(data,coeff,numalts=SAMPLE_SIZE,returnprobs=1)\n",
      "            pdf['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "                \n",
      "        new_homes = pd.Series(np.ones(len(movers.index))*-1,index=movers.index)\n",
      "        mask = np.zeros(len(alts.index),dtype='bool')\n",
      "        \n",
      "        for name, segment in segments:\n",
      "            name = str(name)\n",
      "            print \"Assigning units to %d agents of segment %s\" % (len(segment.index),name)\n",
      "            p=pdf['segment%s'%name].values\n",
      "            #p=pdf['segment%s'%name].values\n",
      "            def choose(p,mask,alternatives,segment,new_homes,minsize=None):\n",
      "                p = copy.copy(p)\n",
      "                p[alternatives.supply<minsize] = 0\n",
      "                #print \"Choosing from %d nonzero alts\" % np.count_nonzero(p)\n",
      "                try: \n",
      "                  indexes = np.random.choice(len(alternatives.index),len(segment.index),replace=False,p=p/p.sum())\n",
      "                except:\n",
      "                  print \"WARNING: not enough options to fit agents, will result in unplaced agents\"\n",
      "                  return mask,new_homes\n",
      "                new_homes.ix[segment.index] = alternatives.index.values[indexes]\n",
      "                alternatives[\"supply\"].ix[alternatives.index.values[indexes]] -= minsize\n",
      "                return mask,new_homes\n",
      "            tmp = segment['employees']\n",
      "            #tmp /= 100.0 ##If scaling demand amount is desired\n",
      "            for name, subsegment in reversed(list(segment.groupby(tmp.astype('int')))):\n",
      "                #print \"Running subsegment with size = %s, num agents = %d\" % (name, len(subsegment.index))\n",
      "                mask,new_homes = choose(p,mask,alts,subsegment,new_homes,minsize=int(name))\n",
      "        \n",
      "        build_cnts = new_homes.value_counts()  #num estabs place in each building\n",
      "        print \"Assigned %d agents to %d locations with %d unplaced\" % (new_homes.size,build_cnts.size,build_cnts.get(-1,0))\n",
      "        \n",
      "        table = dset.establishments # need to go back to the whole dataset\n",
      "        table[depvar].ix[new_homes.index] = new_homes.values.astype('int32')\n",
      "        dset.store_attr(output_varname,year,copy.deepcopy(table[depvar]))\n",
      "        \n",
      "        \n",
      "    #################     HLCM\n",
      "        output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-hlcm-%s.csv\",\"PARIS HOUSEHOLD LOCATION CHOICE MODELS (%s)\",\"hh_location_%s\",\"household_building_ids\")\n",
      "        new_hhlds = {\"table\": \"dset.households\",\"writetotmp\": \"households\",\"model\": \"transitionmodel\",\"first_year\": 1999,\"control_totals\": \"dset.annual_household_control_totals\",\n",
      "                     \"geography_field\": \"building_id\",\"amount_field\": \"total_number_of_households\"}\n",
      "        import synthicity.urbansim.transitionmodel as transitionmodel\n",
      "        transitionmodel.simulate(dset,new_hhlds,year=sim_year,show=True)\n",
      "        year = sim_year\n",
      "        choosers = dset.fetch('households')\n",
      "        depvar = 'building_id'\n",
      "        rate_table = dset.annual_household_relocation_rates\n",
      "        rate_table = rate_table*.1\n",
      "        rate_field = \"probability_of_relocating\"\n",
      "        movers = dset.relocation_rates(choosers,rate_table,rate_field)\n",
      "        choosers[depvar].ix[movers] = -1\n",
      "        movers = choosers[choosers[depvar]==-1]\n",
      "        print \"Total new agents and movers = %d\" % len(movers.index)\n",
      "        alternatives = dset.buildings[(dset.buildings.residential_units>0)]\n",
      "        empty_units = dset.buildings[(dset.buildings.residential_units>0)].residential_units.sub(choosers.groupby('building_id').size(),fill_value=0)\n",
      "        empty_units = empty_units[empty_units>0].order(ascending=False)\n",
      "        alternatives = alternatives.ix[np.repeat(empty_units.index,empty_units.values.astype('int'))]\n",
      "        alts1 = alternatives[alternatives.building_type_id==1]\n",
      "        alts2 = alternatives[alternatives.building_type_id==2]\n",
      "        alts3 = alternatives[alternatives.building_type_id==3]\n",
      "        alts4 = alternatives[alternatives.building_type_id==4]\n",
      "        pdf1 = pd.DataFrame(index=alts1.index)\n",
      "        pdf2 = pd.DataFrame(index=alts2.index) \n",
      "        pdf3 = pd.DataFrame(index=alts3.index)\n",
      "        pdf4 = pd.DataFrame(index=alts4.index)\n",
      "        \n",
      "        #segments = movers.groupby(['btype_tenure',])\n",
      "        #choosers.groupby(['btype_tenure','income_cat','age_cat','sex_of_head'])\n",
      "        segments = movers.groupby(['btype_tenure','low_inc','old','race_id','with_child','with_car']) \n",
      "        \n",
      "        ind_vars1=[\"ln_residential_units_owner_house\",'in_paris','in_paris_suburbs','cd_chatelet','rail_stations_x_0car','rail_stations_x_1car','rail_stations_x_2pluscar',\n",
      "                   'subway_stations_x_0car','subway_stations_x_1car','subway_stations_x_2pluscar','cnoise','cpbois','cpparc_jardin','cpeau','perc_gardens_x_children',\n",
      "                   'ln_price1_x_low_income','ln_price1_x_mid_income','ln_price1_x_high_income','percent_middle_age_x_high_inc','perc_foreign_x_french','perc_foreign_x_foreign','low_inc_x_percent_low_inc','mid_inc_x_percent_mid_inc',\n",
      "                   'high_inc_x_percent_high_inc','hhsize2_x_percent_hhsize2','hhsize3plus_x_percent_hhsize3plus','perc_young_x_young','perc_middle_age_x_middle_age','perc_old_x_old',\n",
      "                   'perc_with_child_x_child_in_hh','tco_x_0car','tco_x_1car','tco_x_2pluscar','vpo_x_0car','vpo_x_1car','vpo_x_2pluscar',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars2=[\"ln_residential_units_owner_flat\",'in_paris','in_paris_suburbs','cd_chatelet','rail_stations_x_0car','rail_stations_x_1car','rail_stations_x_2pluscar',\n",
      "                   'subway_stations_x_0car','subway_stations_x_1car','subway_stations_x_2pluscar','cnoise','cpbois','cpparc_jardin','cpeau','perc_gardens_x_children',\n",
      "                   'ln_price2_x_low_income','ln_price2_x_mid_income','perc_foreign_x_french','perc_foreign_x_foreign','low_inc_x_percent_low_inc','mid_inc_x_percent_mid_inc',\n",
      "                   'high_inc_x_percent_high_inc','hhsize2_x_percent_hhsize2','hhsize3plus_x_percent_hhsize3plus','perc_young_x_young','perc_middle_age_x_middle_age','perc_old_x_old',\n",
      "                   'perc_with_child_x_child_in_hh','tco_x_0car','tco_x_1car','tco_x_2pluscar','vpo_x_0car','vpo_x_1car','vpo_x_2pluscar',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars3=[\"ln_residential_units_renter_house\",'in_paris','in_paris_suburbs','cd_chatelet','rail_stations_x_0car','rail_stations_x_1car','rail_stations_x_2pluscar',\n",
      "                   'subway_stations_x_0car','subway_stations_x_1car','subway_stations_x_2pluscar','cnoise','cpbois','cpparc_jardin','cpeau','perc_gardens_x_children',\n",
      "                   'ln_price3_x_low_income','ln_price3_x_mid_income','ln_price3_x_high_income','perc_foreign_x_french','perc_foreign_x_foreign','low_inc_x_percent_low_inc','mid_inc_x_percent_mid_inc',\n",
      "                   'high_inc_x_percent_high_inc','hhsize2_x_percent_hhsize2','hhsize3plus_x_percent_hhsize3plus','perc_young_x_young','perc_middle_age_x_middle_age','perc_old_x_old',\n",
      "                   'perc_with_child_x_child_in_hh','percent_old_x_low_inc','tco_x_0car','tco_x_1car','tco_x_2pluscar','vpo_x_0car','vpo_x_1car','vpo_x_2pluscar',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars4=[\"ln_residential_units_renter_flat\",'in_paris','in_paris_suburbs','cd_chatelet','rail_stations_x_0car','rail_stations_x_1car','rail_stations_x_2pluscar',\n",
      "                   'subway_stations_x_0car','subway_stations_x_1car','subway_stations_x_2pluscar','cnoise','cpbois','cpparc_jardin','cpeau','perc_gardens_x_children',\n",
      "                   'ln_price4_x_low_income','ln_price4_x_mid_income','perc_foreign_x_french','perc_foreign_x_foreign','low_inc_x_percent_low_inc','mid_inc_x_percent_mid_inc',\n",
      "                   'high_inc_x_percent_high_inc','hhsize2_x_percent_hhsize2','hhsize3plus_x_percent_hhsize3plus','perc_young_x_young','perc_middle_age_x_middle_age','perc_old_x_old',\n",
      "                   'perc_with_child_x_child_in_hh','percent_middle_age_x_mid_inc','tco_x_0car','tco_x_1car','tco_x_2pluscar','vpo_x_0car','vpo_x_1car','vpo_x_2pluscar',\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99'] \n",
      "    \n",
      "        for name, segment in segments:\n",
      "            if type(name) is np.int64:  name = (name,0)\n",
      "            if name[0] == 1:\n",
      "                alts = alts1\n",
      "                ind_vars = ind_vars1\n",
      "            if name[0] == 2:\n",
      "                alts = alts2\n",
      "                ind_vars = ind_vars2\n",
      "            if name[0] == 3:\n",
      "                alts = alts3\n",
      "                ind_vars = ind_vars3\n",
      "            if name[0] == 4:\n",
      "                alts = alts4\n",
      "                ind_vars = ind_vars4\n",
      "            segment = segment.head(1)\n",
      "            name_coeff = str(name[0])\n",
      "            name = str(name)\n",
      "            tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name_coeff\n",
      "            SAMPLE_SIZE = alts.index.size \n",
      "            numchoosers = segment.shape[0]\n",
      "            numalts = alts.shape[0]\n",
      "            sample = np.tile(alts.index.values,numchoosers)\n",
      "            alts_sample = alts #sample#alternatives\n",
      "            alts_sample['join_index'] = np.repeat(segment.index,SAMPLE_SIZE)\n",
      "            alts_sample = pd.merge(alts_sample,segment,left_on='join_index',right_index=True,suffixes=('','_r'))\n",
      "            chosen = np.zeros((numchoosers,SAMPLE_SIZE))\n",
      "            chosen[:,0] = 1\n",
      "            sample, alternative_sample, est_params = sample, alts_sample, ('mnl',chosen)\n",
      "            alternative_sample['high_inc_x_percent_high_inc'] = (alternative_sample.high_inc*alternative_sample.percent_high_income)\n",
      "            alternative_sample['mid_inc_x_percent_mid_inc'] = (alternative_sample.mid_inc*alternative_sample.percent_mid_income)\n",
      "            alternative_sample['low_inc_x_percent_low_inc'] = (alternative_sample.low_inc*alternative_sample.percent_low_income)\n",
      "            alternative_sample['hhsize3plus_x_percent_hhsize3plus'] = (alternative_sample.hhsize3plus*alternative_sample.percent_hhsize3plus)\n",
      "            alternative_sample['hhsize2_x_percent_hhsize2'] = (alternative_sample.hhsize2*alternative_sample.percent_hhsize2)\n",
      "            #alternative_sample['same_dpt_as_previous'] = (alternative_sample.previous_dpt==alternative_sample.dept_id).astype('int32')\n",
      "            alternative_sample['rail_stations_x_0car'] = (alternative_sample.ctrain9*(alternative_sample.cars==0)).astype('int32')\n",
      "            alternative_sample['rail_stations_x_1car'] = (alternative_sample.ctrain9*(alternative_sample.cars==1)).astype('int32')\n",
      "            alternative_sample['rail_stations_x_2pluscar'] = (alternative_sample.ctrain9*(alternative_sample.cars>1)).astype('int32')\n",
      "            alternative_sample['subway_stations_x_0car'] = (alternative_sample.csubway9*(alternative_sample.cars==0)).astype('int32')\n",
      "            alternative_sample['subway_stations_x_1car'] = (alternative_sample.csubway9*(alternative_sample.cars==1)).astype('int32')\n",
      "            alternative_sample['subway_stations_x_2pluscar'] = (alternative_sample.csubway9*(alternative_sample.cars>1)).astype('int32')\n",
      "            alternative_sample['tco_x_0car'] = (alternative_sample.tco*(alternative_sample.cars==0)).astype('int32')\n",
      "            alternative_sample['tco_x_1car'] = (alternative_sample.tco*(alternative_sample.cars==1)).astype('int32')\n",
      "            alternative_sample['tco_x_2pluscar'] = (alternative_sample.tco*(alternative_sample.cars>1)).astype('int32')\n",
      "            alternative_sample['vpo_x_0car'] = (alternative_sample.vpo*(alternative_sample.cars==0)).astype('int32')\n",
      "            alternative_sample['vpo_x_1car'] = (alternative_sample.vpo*(alternative_sample.cars==1)).astype('int32')\n",
      "            alternative_sample['vpo_x_2pluscar'] = (alternative_sample.vpo*(alternative_sample.cars>1)).astype('int32')\n",
      "            alternative_sample['perc_gardens_x_children'] = (alternative_sample.cpparc_jardin*alternative_sample.children).astype('int32')\n",
      "            alternative_sample['perc_foreign_x_french'] = (alternative_sample.percent_foreigners*(alternative_sample.race_id==0))\n",
      "            alternative_sample['perc_foreign_x_foreign'] = (alternative_sample.percent_foreigners*(alternative_sample.race_id==1))\n",
      "            alternative_sample['perc_young_x_young'] = (alternative_sample.percent_young*alternative_sample.young)\n",
      "            alternative_sample['perc_middle_age_x_middle_age'] = (alternative_sample.percent_middle_age*alternative_sample.middle_age)\n",
      "            alternative_sample['perc_old_x_old'] = (alternative_sample.percent_old*alternative_sample.old)\n",
      "            alternative_sample['perc_with_child_x_child_in_hh'] = (alternative_sample.with_child*alternative_sample.percent_with_child)\n",
      "            alternative_sample['percent_middle_age_x_high_inc'] = (alternative_sample.high_inc*alternative_sample.percent_middle_age)\n",
      "            alternative_sample['percent_old_x_low_inc'] = (alternative_sample.low_inc*alternative_sample.percent_old)\n",
      "            alternative_sample['percent_middle_age_x_mid_inc'] = (alternative_sample.mid_inc*alternative_sample.percent_middle_age)\n",
      "            if int(name_coeff)==1:\n",
      "                alternative_sample['ln_price1_x_low_income'] = (alternative_sample.ln_average_res_price1*alternative_sample.low_inc)\n",
      "                alternative_sample['ln_price1_x_mid_income'] = (alternative_sample.ln_average_res_price1*alternative_sample.mid_inc)\n",
      "                alternative_sample['ln_price1_x_high_income'] = (alternative_sample.ln_average_res_price1*alternative_sample.high_inc)\n",
      "            if int(name_coeff)==2:\n",
      "                alternative_sample['ln_price2_x_low_income'] = (alternative_sample.ln_average_res_price2*alternative_sample.low_inc)\n",
      "                alternative_sample['ln_price2_x_mid_income'] = (alternative_sample.ln_average_res_price2*alternative_sample.mid_inc)\n",
      "                alternative_sample['ln_price2_x_high_income'] = (alternative_sample.ln_average_res_price2*alternative_sample.high_inc)\n",
      "            if int(name_coeff)==3:\n",
      "                alternative_sample['ln_price3_x_low_income'] = (alternative_sample.ln_average_res_price3*alternative_sample.low_inc)\n",
      "                alternative_sample['ln_price3_x_mid_income'] = (alternative_sample.ln_average_res_price3*alternative_sample.mid_inc)\n",
      "                alternative_sample['ln_price3_x_high_income'] = (alternative_sample.ln_average_res_price3*alternative_sample.high_inc)\n",
      "            if int(name_coeff)==4:\n",
      "                alternative_sample['ln_price4_x_low_income'] = (alternative_sample.ln_average_res_price4*alternative_sample.low_inc)\n",
      "                alternative_sample['ln_price4_x_mid_income'] = (alternative_sample.ln_average_res_price4*alternative_sample.mid_inc)\n",
      "                alternative_sample['ln_price4_x_high_income'] = (alternative_sample.ln_average_res_price4*alternative_sample.high_inc)\n",
      "            est_data = pd.DataFrame(index=alternative_sample.index)\n",
      "            for varname in ind_vars:\n",
      "                est_data[varname] = alternative_sample[varname]\n",
      "            est_data = est_data.fillna(0)\n",
      "            data = est_data\n",
      "            data = data.as_matrix()\n",
      "            coeff = dset.load_coeff(tmp_coeffname)\n",
      "            probs = interaction.mnl_simulate(data,coeff,numalts=SAMPLE_SIZE,returnprobs=1)\n",
      "            if int(name_coeff) == 1:\n",
      "                pdf1['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index)  \n",
      "            if int(name_coeff) == 2:\n",
      "                pdf2['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "            if int(name_coeff) == 3:\n",
      "                pdf3['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "            if int(name_coeff) == 4:\n",
      "                pdf4['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "     \n",
      "        new_homes = pd.Series(np.ones(len(movers.index))*-1,index=movers.index)\n",
      "        #mask = np.zeros(len(alternatives.index),dtype='bool')\n",
      "        for name, segment in segments:\n",
      "            if type(name) is np.int64:  name = (name,0)\n",
      "            name_coeff = str(name[0])\n",
      "            name = str(name)\n",
      "            if int(name_coeff) == 1:\n",
      "                p=pdf1['segment%s'%name].values\n",
      "                mask = np.zeros(len(alts1.index),dtype='bool')\n",
      "            if int(name_coeff) == 2:\n",
      "                p=pdf2['segment%s'%name].values \n",
      "                mask = np.zeros(len(alts2.index),dtype='bool')\n",
      "            if int(name_coeff) == 3:\n",
      "                p=pdf3['segment%s'%name].values \n",
      "                mask = np.zeros(len(alts3.index),dtype='bool')\n",
      "            if int(name_coeff) == 4:\n",
      "                p=pdf4['segment%s'%name].values\n",
      "                mask = np.zeros(len(alts4.index),dtype='bool')\n",
      "            print \"Assigning units to %d agents of segment %s\" % (len(segment.index),name)\n",
      "            #p=pdf['segment%s'%name].values\n",
      "         \n",
      "            def choose(p,mask,alternatives,segment,new_homes,minsize=None):\n",
      "                p = copy.copy(p)\n",
      "                p[mask] = 0 # already chosen\n",
      "                #print \"Choosing from %d nonzero alts\" % np.count_nonzero(p)\n",
      "                try: \n",
      "                  indexes = np.random.choice(len(alternatives.index),len(segment.index),replace=False,p=p/p.sum())\n",
      "                except:\n",
      "                  print \"WARNING: not enough options to fit agents, will result in unplaced agents\"\n",
      "                  return mask,new_homes\n",
      "                new_homes.ix[segment.index] = alternatives.index.values[indexes]\n",
      "                mask[indexes] = 1\n",
      "              \n",
      "                return mask,new_homes\n",
      "            if int(name_coeff) == 1:\n",
      "                mask,new_homes = choose(p,mask,alts1,segment,new_homes)\n",
      "            if int(name_coeff) == 2:\n",
      "                mask,new_homes = choose(p,mask,alts2,segment,new_homes)\n",
      "            if int(name_coeff) == 3:\n",
      "                mask,new_homes = choose(p,mask,alts3,segment,new_homes)\n",
      "            if int(name_coeff) == 4:\n",
      "                mask,new_homes = choose(p,mask,alts4,segment,new_homes)\n",
      "            \n",
      "        build_cnts = new_homes.value_counts()  #num households place in each building\n",
      "        print \"Assigned %d agents to %d locations with %d unplaced\" % (new_homes.size,build_cnts.size,build_cnts.get(-1,0))\n",
      "        \n",
      "        table = dset.households # need to go back to the whole dataset\n",
      "        table[depvar].ix[new_homes.index] = new_homes.values.astype('int32')\n",
      "        dset.store_attr(output_varname,year,copy.deepcopy(table[depvar]))\n",
      "        \n",
      "        \n",
      "    #################     RDPLCM\n",
      "        target_vacancy1 = .081\n",
      "        target_vacancy2 = .081\n",
      "        target_vacancy3 = .081\n",
      "        target_vacancy4 = .081\n",
      "        target_vacancy6 = .097\n",
      "        target_vacancies = pd.Series([target_vacancy1,target_vacancy2,target_vacancy3,target_vacancy4,target_vacancy6],index=[1,2,3,4,6])\n",
      "        households_by_btype = dset.households.groupby('btype_tenure').building_id.count()\n",
      "        resunits_by_btype = dset.buildings[dset.buildings.building_type_id<5].groupby('building_type_id').residential_units.sum()\n",
      "        vacant_resunits = resunits_by_btype - households_by_btype\n",
      "        vacant_resunits = vacant_resunits[vacant_resunits.index.values<6]\n",
      "        target_vacant_resunits = resunits_by_btype * target_vacancies\n",
      "        diff_resunits = np.round(target_vacant_resunits - vacant_resunits)\n",
      "        print 'Residential units by building type to construct:  '\n",
      "        building_type_ids = []\n",
      "        building_ids = []\n",
      "        for idx_btype in diff_resunits[diff_resunits>0].index:\n",
      "            building_type_id = idx_btype\n",
      "            residential_units_to_build = int(diff_resunits[idx_btype])\n",
      "            print building_type_id, residential_units_to_build\n",
      "            building_type_ids += [building_type_id]*residential_units_to_build\n",
      "            building_ids += [-1]*residential_units_to_build\n",
      "        building_type_ids = np.array(building_type_ids)\n",
      "        building_ids = np.array(building_ids)\n",
      "        residential_unit_ids = np.arange(len(building_ids))+1\n",
      "        residential_units = pd.DataFrame({'building_type_id':building_type_ids,'building_id':building_ids,'residential_unit_id':residential_unit_ids})\n",
      "        residential_units = residential_units.set_index('residential_unit_id')\n",
      "        output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-dplcm-%s.csv\",\"PARIS DEVPROJECT LOCATION CHOICE MODELS (%s)\",\"dp_location_%s\",\"devproject_building_ids\")\n",
      "        year = sim_year\n",
      "        choosers = residential_units\n",
      "        depvar = 'building_id'\n",
      "        movers = choosers[choosers[depvar]==-1]\n",
      "        print \"Total new agents and movers = %d\" % len(movers.index)\n",
      "        alternatives = dset.buildings[(dset.buildings.residential_units_capacity>0)]\n",
      "        empty_units = dset.buildings[(dset.buildings.residential_units_capacity>0)].residential_units_capacity.sub(dset.buildings.residential_units,fill_value=0)\n",
      "        empty_units = empty_units[empty_units>0].order(ascending=False)\n",
      "        empty_units[empty_units>2000] = 2000\n",
      "        ind_vars1=['in_new_town','in_paris','in_paris_suburbs','distance_to_arterial','distance_to_highway','cd_chatelet','ln_land_area','csubway9','ctrain9',\n",
      "                   'percent_high_income','percent_low_income','cnoise','cpbois','cpequipem_sante','cpparc_jardin','cpsport','cpeau','cpraft90','percent_hh_one_worker',\n",
      "                   'percent_hh_twoplus_workers','cprbef15','percent_foreigners','percent_young','percent_old','percent_hhsize2', 'percent_hhsize3plus',\n",
      "                   'employment_density','population_density','vpo',] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars2=['in_new_town','in_paris','in_paris_suburbs','distance_to_arterial','distance_to_highway','cd_chatelet','ln_land_area','csubway9','ctrain9',\n",
      "                   'percent_high_income','percent_low_income','cnoise','cpbois','cpequipem_sante','cpparc_jardin','cpsport','cpeau','cpraft90','percent_hh_one_worker',\n",
      "                   'percent_hh_twoplus_workers','cprbef15','percent_foreigners','percent_young','percent_old','percent_hhsize2', 'percent_hhsize3plus',\n",
      "                   'employment_density','population_density','vpo',] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars3=['in_new_town','in_paris','in_paris_suburbs','distance_to_arterial','distance_to_highway','cd_chatelet','ln_land_area','csubway9','ctrain9',\n",
      "                   'percent_high_income','percent_low_income','cnoise','cpbois','cpequipem_sante','cpparc_jardin','cpsport','cpeau','cpraft90','percent_hh_one_worker',\n",
      "                   'percent_hh_twoplus_workers','cprbef15','percent_foreigners','percent_young','percent_old','percent_hhsize2', 'percent_hhsize3plus',\n",
      "                   'employment_density','population_density','vpo',] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        ind_vars4=['in_new_town','in_paris','in_paris_suburbs','distance_to_arterial','distance_to_highway','cd_chatelet','ln_land_area','csubway9','ctrain9',\n",
      "                   'percent_high_income','percent_low_income','cnoise','cpbois','cpequipem_sante','cpparc_jardin','cpsport','cpeau','cpraft90','percent_hh_one_worker',\n",
      "                   'percent_hh_threeplus_workers','cprbef15','percent_foreigners','percent_young','percent_old','percent_hhsize2', 'percent_hhsize3plus',\n",
      "                   'employment_density','population_density','vpo',] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        indvars_together = ind_vars1 + ind_vars2 + ind_vars3 + ind_vars4 + ['building_type_id','zone_id','zgp_id','dept_id','residential_units','residential_units_capacity','non_residential_sqft','non_residential_sqft_capacity']\n",
      "        columns_to_keep = np.unique(indvars_together)\n",
      "        alternatives = alternatives[list(columns_to_keep)]\n",
      "        alternatives = alternatives.ix[np.repeat(empty_units.index,empty_units.values.astype('int'))]\n",
      "        alts1 = alternatives[alternatives.building_type_id==1]\n",
      "        alts2 = alternatives[alternatives.building_type_id==2]\n",
      "        alts3 = alternatives[alternatives.building_type_id==3]\n",
      "        alts4 = alternatives[alternatives.building_type_id==4]\n",
      "        \n",
      "        segments = movers.groupby(['building_type_id',])\n",
      "        \n",
      "        for name, segment in segments:\n",
      "            if name == 1:\n",
      "                alts = alts1\n",
      "                ind_vars = ind_vars1\n",
      "                pdf1 = pd.DataFrame(index=alts1.index) \n",
      "            if name == 2:\n",
      "                alts = alts2\n",
      "                ind_vars = ind_vars2\n",
      "                pdf2 = pd.DataFrame(index=alts2.index) \n",
      "            if name == 3:\n",
      "                alts = alts3\n",
      "                ind_vars = ind_vars3\n",
      "                pdf3 = pd.DataFrame(index=alts3.index) \n",
      "            if name == 4:\n",
      "                alts = alts4\n",
      "                ind_vars = ind_vars4\n",
      "                pdf4 = pd.DataFrame(index=alts4.index) \n",
      "            \n",
      "            segment = segment.head(1)\n",
      "            name = str(name)\n",
      "            tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name\n",
      "            SAMPLE_SIZE = alts.index.size \n",
      "            numchoosers = segment.shape[0]\n",
      "            numalts = alts.shape[0]\n",
      "            sample = np.tile(alts.index.values,numchoosers)\n",
      "            alts_sample = alts #sample#alternatives\n",
      "            alts_sample['join_index'] = np.repeat(segment.index,SAMPLE_SIZE)\n",
      "            alts_sample = pd.merge(alts_sample,segment,left_on='join_index',right_index=True,suffixes=('','_r'))\n",
      "            chosen = np.zeros((numchoosers,SAMPLE_SIZE))\n",
      "            chosen[:,0] = 1\n",
      "            sample, alternative_sample, est_params = sample, alts_sample, ('mnl',chosen)\n",
      "            est_data = pd.DataFrame(index=alternative_sample.index)\n",
      "            for varname in ind_vars:\n",
      "                est_data[varname] = alternative_sample[varname]\n",
      "            est_data = est_data.fillna(0)\n",
      "            data = est_data\n",
      "            data = data.as_matrix()\n",
      "            coeff = dset.load_coeff(tmp_coeffname)\n",
      "            probs = interaction.mnl_simulate(data,coeff,numalts=SAMPLE_SIZE,returnprobs=1)\n",
      "            if int(name) == 1:\n",
      "                pdf1['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index)  \n",
      "            if int(name) == 2:\n",
      "                pdf2['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "            if int(name) == 3:\n",
      "                pdf3['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "            if int(name) == 4:\n",
      "                pdf4['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "     \n",
      "        new_homes = pd.Series(np.ones(len(movers.index))*-1,index=movers.index)\n",
      "        #mask = np.zeros(len(alternatives.index),dtype='bool')\n",
      "        for name, segment in segments:\n",
      "            name = str(name)\n",
      "            if int(name) == 1:\n",
      "                p=pdf1['segment%s'%name].values\n",
      "                mask = np.zeros(len(alts1.index),dtype='bool')\n",
      "            if int(name) == 2:\n",
      "                p=pdf2['segment%s'%name].values \n",
      "                mask = np.zeros(len(alts2.index),dtype='bool')\n",
      "            if int(name) == 3:\n",
      "                p=pdf3['segment%s'%name].values \n",
      "                mask = np.zeros(len(alts3.index),dtype='bool')\n",
      "            if int(name) == 4:\n",
      "                p=pdf4['segment%s'%name].values\n",
      "                mask = np.zeros(len(alts4.index),dtype='bool')\n",
      "            print \"Assigning units to %d agents of segment %s\" % (len(segment.index),name)\n",
      "            #p=pdf['segment%s'%name].values\n",
      "         \n",
      "            def choose(p,mask,alternatives,segment,new_homes,minsize=None):\n",
      "                p = copy.copy(p)\n",
      "                if minsize is not None: p[alternatives.supply<minsize] = 0\n",
      "                else: p[mask] = 0 # already chosen\n",
      "    #             print \"Choosing from %d nonzero alts\" % np.count_nonzero(p)\n",
      "                try: \n",
      "                  indexes = np.random.choice(len(alternatives.index),len(segment.index),replace=False,p=p/p.sum())\n",
      "                except:\n",
      "                  print \"WARNING: not enough options to fit agents, will result in unplaced agents\"\n",
      "                  return mask,new_homes\n",
      "                new_homes.ix[segment.index] = alternatives.index.values[indexes]\n",
      "            \n",
      "                if minsize is not None: alternatives[\"supply\"].ix[alternatives.index.values[indexes]] -= minsize\n",
      "                else: mask[indexes] = 1\n",
      "              \n",
      "                return mask,new_homes\n",
      "            if int(name) == 1:\n",
      "                mask,new_homes = choose(p,mask,alts1,segment,new_homes)\n",
      "            if int(name) == 2:\n",
      "                mask,new_homes = choose(p,mask,alts2,segment,new_homes)\n",
      "            if int(name) == 3:\n",
      "                mask,new_homes = choose(p,mask,alts3,segment,new_homes)\n",
      "            if int(name) == 4:\n",
      "                mask,new_homes = choose(p,mask,alts4,segment,new_homes)\n",
      "            \n",
      "        build_cnts = new_homes.value_counts()  #num resunits place in each building\n",
      "        print \"Assigned %d agents to %d locations with %d unplaced\" % (new_homes.size,build_cnts.size,build_cnts.get(-1,0))\n",
      "        \n",
      "        table = residential_units # need to go back to the whole dataset *****************\n",
      "        table[depvar].ix[new_homes.index] = new_homes.values.astype('int32')\n",
      "        dset.store_attr(output_varname,year,copy.deepcopy(table[depvar]))\n",
      "        new_res_construction_totals = residential_units.groupby('building_id').size()\n",
      "        print 'Previous residential unit total:'\n",
      "        print dset.buildings.residential_units.sum()\n",
      "        dset.buildings.residential_units[np.in1d(dset.buildings.index,new_res_construction_totals.index)] = dset.buildings.residential_units[np.in1d(dset.buildings.index,new_res_construction_totals.index)] + new_res_construction_totals\n",
      "        print 'Current residential unit total:'\n",
      "        print dset.buildings.residential_units.sum()\n",
      "        \n",
      "        \n",
      "    #################     NRDPLCM\n",
      "        target_vacancy6 = .097\n",
      "        employment_by_building = dset.establishments.groupby('building_id').employees.sum()\n",
      "        bsqft_job = dset.buildings[dset.buildings.building_type_id==6].building_sqft_per_job\n",
      "        occupied_nonres_sqft = employment_by_building*bsqft_job\n",
      "        total_occupied_nonres_sqft = occupied_nonres_sqft.sum()\n",
      "        total_nonres_sqft = dset.buildings.non_residential_sqft.sum()\n",
      "        vacant_nonres_sqft = total_nonres_sqft - total_occupied_nonres_sqft\n",
      "        target_vacant_nonres_sqft = total_nonres_sqft*target_vacancy6\n",
      "        nonres_sqft_to_build = target_vacant_nonres_sqft - vacant_nonres_sqft\n",
      "        print 'Non-residential sqft to construct:  '\n",
      "        print nonres_sqft_to_build\n",
      "        nonres_units_to_build = int(round(nonres_sqft_to_build/500.0))\n",
      "        print 'Non-residential units to construct:  '\n",
      "        print nonres_units_to_build\n",
      "        building_type_ids = [6]*nonres_units_to_build\n",
      "        nonres_building_ids = [-1]*nonres_units_to_build\n",
      "        building_type_ids = np.array(building_type_ids)\n",
      "        nonres_building_ids = np.array(nonres_building_ids)\n",
      "        nonres_unit_ids = np.arange(len(nonres_building_ids))+1\n",
      "        nonres_units = pd.DataFrame({'building_type_id':building_type_ids,'building_id':nonres_building_ids,'nonres_unit_id':nonres_unit_ids})\n",
      "        nonres_units = nonres_units.set_index('nonres_unit_id')\n",
      "        output_csv, output_title, coeff_name, output_varname = (\"paris-coeff-dplcm-%s.csv\",\"PARIS DEVPROJECT LOCATION CHOICE MODELS (%s)\",\"dp_location_%s\",\"devproject_building_ids\")\n",
      "        year = sim_year\n",
      "        choosers = nonres_units\n",
      "        depvar = 'building_id'\n",
      "        movers = choosers[choosers[depvar]==-1]\n",
      "        print \"Total new agents and movers = %d\" % len(movers.index)\n",
      "        alternatives = dset.buildings[(dset.buildings.non_residential_sqft_capacity>0)]\n",
      "        alternatives['nonres_units'] = alternatives.non_residential_sqft/500\n",
      "        alternatives['nonres_units_capacity'] = alternatives.non_residential_sqft_capacity/500\n",
      "        empty_units = alternatives.nonres_units_capacity.sub(alternatives.nonres_units,fill_value=0)\n",
      "        empty_units = empty_units[empty_units>0].order(ascending=False)\n",
      "        empty_units[empty_units>2500] = 2500\n",
      "        ind_vars6=['in_la_defense','in_new_town','in_paris','in_paris_suburbs','distance_to_arterial','distance_to_highway','cd_chatelet','ln_land_area','csubway9','ctrain9',\n",
      "                   'percent_high_income','percent_low_income','cnoise','cpraft90','percent_hh_one_worker','percent_hh_twoplus_workers','cprbef15','percent_foreigners','percent_young','percent_old',\n",
      "                   'percent_hhsize2', 'percent_hhsize3plus','employment_density','population_density','vpo'\n",
      "                   ] + ['zgpgroup21','zgpgroup22','zgpgroup23','zgpgroup24','zgpgroup25','zgpgroup26','zgpgroup47','zgpgroup48','zgpgroup49','zgpgroup50','zgpgroup75','zgpgroup99']\n",
      "        indvars_together = ind_vars6 + ['building_type_id','zone_id','zgp_id','dept_id','residential_units','residential_units_capacity','non_residential_sqft','non_residential_sqft_capacity']\n",
      "        columns_to_keep = np.unique(indvars_together)\n",
      "        alternatives = alternatives[list(columns_to_keep)]\n",
      "        alts6 = alternatives.ix[np.repeat(empty_units.index,empty_units.values.astype('int'))]\n",
      "        \n",
      "        segments = movers.groupby(['building_type_id',])\n",
      "        \n",
      "        for name, segment in segments:\n",
      "            if name == 6:\n",
      "                alts = alts6\n",
      "                ind_vars = ind_vars6\n",
      "                pdf6 = pd.DataFrame(index=alts.index) \n",
      "            \n",
      "            segment = segment.head(1)\n",
      "            name = str(name)\n",
      "            tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name\n",
      "            SAMPLE_SIZE = alts.index.size \n",
      "            numchoosers = segment.shape[0]\n",
      "            numalts = alts.shape[0]\n",
      "            sample = np.tile(alts.index.values,numchoosers)\n",
      "            alts_sample = alts #sample#alternatives\n",
      "            alts_sample['join_index'] = np.repeat(segment.index,SAMPLE_SIZE)\n",
      "            alts_sample = pd.merge(alts_sample,segment,left_on='join_index',right_index=True,suffixes=('','_r'))\n",
      "            chosen = np.zeros((numchoosers,SAMPLE_SIZE))\n",
      "            chosen[:,0] = 1\n",
      "            sample, alternative_sample, est_params = sample, alts_sample, ('mnl',chosen)\n",
      "            est_data = pd.DataFrame(index=alternative_sample.index)\n",
      "            for varname in ind_vars:\n",
      "                est_data[varname] = alternative_sample[varname]\n",
      "            est_data = est_data.fillna(0)\n",
      "            data = est_data\n",
      "            data = data.as_matrix()\n",
      "            coeff = dset.load_coeff(tmp_coeffname)\n",
      "            probs = interaction.mnl_simulate(data,coeff,numalts=SAMPLE_SIZE,returnprobs=1)\n",
      "            if int(name) == 6:\n",
      "                pdf6['segment%s'%name] = pd.Series(probs.flatten(),index=alts.index) \n",
      "        new_homes = pd.Series(np.ones(len(movers.index))*-1,index=movers.index)\n",
      "        for name, segment in segments:\n",
      "            name = str(name)\n",
      "            if int(name) == 6:\n",
      "                p=pdf6['segment%s'%name].values\n",
      "                mask = np.zeros(len(alts6.index),dtype='bool')\n",
      "            print \"Assigning units to %d agents of segment %s\" % (len(segment.index),name)\n",
      "         \n",
      "            def choose(p,mask,alternatives,segment,new_homes,minsize=None):\n",
      "                p = copy.copy(p)\n",
      "                if minsize is not None: p[alternatives.supply<minsize] = 0\n",
      "                else: p[mask] = 0 # already chosen\n",
      "                #print \"Choosing from %d nonzero alts\" % np.count_nonzero(p)\n",
      "        \n",
      "                try: \n",
      "                  indexes = np.random.choice(len(alternatives.index),len(segment.index),replace=False,p=p/p.sum())\n",
      "                except:\n",
      "                  print \"WARNING: not enough options to fit agents, will result in unplaced agents\"\n",
      "                  return mask,new_homes\n",
      "                new_homes.ix[segment.index] = alternatives.index.values[indexes]\n",
      "            \n",
      "                if minsize is not None: alternatives[\"supply\"].ix[alternatives.index.values[indexes]] -= minsize\n",
      "                else: mask[indexes] = 1\n",
      "              \n",
      "                return mask,new_homes\n",
      "            if int(name) == 6:\n",
      "                mask,new_homes = choose(p,mask,alts6,segment,new_homes)\n",
      "            \n",
      "        build_cnts = new_homes.value_counts()  #num resunits place in each building\n",
      "        print \"Assigned %d agents to %d locations with %d unplaced\" % (new_homes.size,build_cnts.size,build_cnts.get(-1,0))\n",
      "        \n",
      "        table = nonres_units # need to go back to the whole dataset *****************\n",
      "        table[depvar].ix[new_homes.index] = new_homes.values.astype('int32')\n",
      "        dset.store_attr('nr'+output_varname,year,copy.deepcopy(table[depvar]))\n",
      "        new_nonres_construction_totals = table.groupby('building_id').size()*500\n",
      "        print 'Previous non-residential sqft total:'\n",
      "        print dset.buildings.non_residential_sqft.sum()\n",
      "        dset.buildings.non_residential_sqft[np.in1d(dset.buildings.index,new_nonres_construction_totals.index)] = dset.buildings.non_residential_sqft[np.in1d(dset.buildings.index,new_nonres_construction_totals.index)] + new_nonres_construction_totals\n",
      "        print 'Current non-residential sqft total:'\n",
      "        print dset.buildings.non_residential_sqft.sum()\n",
      "    \n",
      "    \n",
      "    #################     REPM\n",
      "        year = sim_year\n",
      "        buildings = dset.fetch('buildings')\n",
      "        output_csv, output_title, coeff_name, output_varname = [\"paris-coeff-hedonic.csv\",\"PARIS HEDONIC MODEL\",\"price_%s\",\"price\"]\n",
      "        ind_vars1 = ['in_paris_suburbs','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                     'population_density','tco',]\n",
      "        ind_vars2 = ['in_paris','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                     'population_density','tco',]\n",
      "        ind_vars3 = ['in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old',\n",
      "                     'population_density','tco',]\n",
      "        ind_vars4 = ['in_paris','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','percent_old','bati',\n",
      "                     'population_density','tco',]\n",
      "        ind_vars6 = ['in_paris','in_la_defense','in_new_town','in_paris_suburbs','csubway9','percent_low_income','cpraft90','cprbef15','bati',\n",
      "                     'employment_density','tax_on_professionals','tco',]\n",
      "        simrents = []\n",
      "        segments = buildings.groupby('building_type_id')\n",
      "        for name, segment in segments:\n",
      "            if name == 1:\n",
      "                indvars = ind_vars1\n",
      "            if name == 2:\n",
      "                indvars = ind_vars2\n",
      "            if name == 3:\n",
      "                indvars = ind_vars3\n",
      "            if name == 4:\n",
      "                indvars = ind_vars4\n",
      "            if name == 6:\n",
      "                indvars = ind_vars6\n",
      "            est_data = pd.DataFrame(index=segment.index)\n",
      "            for varname in indvars:\n",
      "                est_data[varname] = segment[varname]\n",
      "            est_data = est_data.fillna(0)\n",
      "            est_data = sm.add_constant(est_data,prepend=False)\n",
      "            tmp_outcsv, tmp_outtitle, tmp_coeffname = output_csv%name, output_title%name, coeff_name%name\n",
      "            print \"Generating rents on %d buildings\" % (est_data.shape[0])\n",
      "            vec = dset.load_coeff(tmp_coeffname)\n",
      "            vec = np.reshape(vec,(vec.size,1))\n",
      "            rents = est_data.dot(vec).astype('f4')\n",
      "            rents = rents.apply(np.exp)\n",
      "            simrents.append(rents[rents.columns[0]])\n",
      "            \n",
      "        simrents = pd.concat(simrents)\n",
      "        dset.buildings[output_varname] = simrents.reindex(dset.buildings.index)\n",
      "        dset.store_attr(output_varname,year,simrents)\n",
      "        \n",
      "    #######ANNUAL SUMMARY\n",
      "        b = dset.fetch('buildings')\n",
      "        e = dset.fetch('establishments')\n",
      "        hh = dset.fetch('households')\n",
      "        summary['employment'].append(e[e.building_id>0].employees.sum())\n",
      "        summary['households'].append(len(hh[hh.building_id>0].building_id))\n",
      "        summary['non_residential_sqft'].append(b.non_residential_sqft.sum())\n",
      "        summary['residential_units'].append(b.residential_units.sum())\n",
      "        summary['price'].append(b.price.mean())\n",
      "        \n",
      "        ##End-of-iteration calibration update\n",
      "        if sim_year == last_year:\n",
      "            print summary\n",
      "            z['total_households'] = hh.groupby('zone_id').building_id.count()\n",
      "            z['total_residential_units'] = b.groupby('zone_id').residential_units.sum()\n",
      "            z['total_nonresidential_sqft'] = b.groupby('zone_id').non_residential_sqft.sum()\n",
      "            z['total_employment'] = e.groupby('zone_id').employees.sum()\n",
      "            sim_emp = z.groupby('zgpgroup_id').total_employment.sum()\n",
      "            sim_hh = z.groupby('zgpgroup_id').total_households.sum()\n",
      "            sim_ru = z.groupby('zgpgroup_id').total_residential_units.sum()\n",
      "            sim_nr = z.groupby('zgpgroup_id').total_nonresidential_sqft.sum()\n",
      "            emp_diff = sim_emp - base_emp\n",
      "            hh_diff = sim_hh - base_hh\n",
      "            ru_diff = sim_ru - base_ru\n",
      "            nr_diff = sim_nr - base_nr\n",
      "            prop_growth_emp = emp_diff*1.0/emp_diff.sum()\n",
      "            prop_growth_hh = hh_diff*1.0/hh_diff.sum()\n",
      "            prop_growth_ru = ru_diff*1.0/ru_diff.sum()\n",
      "            prop_growth_nr = nr_diff*1.0/nr_diff.sum()\n",
      "            i = 0;j = 0;k = 0;m = 0\n",
      "            for x in targets.index.values:\n",
      "                cid = int(x)\n",
      "                print cid\n",
      "                prop_ru = prop_growth_ru[cid]\n",
      "                prop_hh = prop_growth_hh[cid]\n",
      "                prop_emp = prop_growth_emp[cid]\n",
      "                prop_nonres = prop_growth_nr[cid]\n",
      "                target_ru = targets.resunit_target[cid]\n",
      "                target_hh = targets.hh_target[cid]\n",
      "                target_emp = targets.emp_target[cid]\n",
      "                target_nonres = targets.nr_target[cid]\n",
      "                print 'ru prop is ' + str(prop_ru)\n",
      "                print 'ru target is ' + str(target_ru)\n",
      "                print 'nsqft prop is ' + str(prop_nonres)\n",
      "                print 'nsqft target is ' + str(target_nonres)\n",
      "                print 'hh prop is ' + str(prop_hh)\n",
      "                print 'hh target is ' + str(target_hh)\n",
      "                print 'emp prop is ' + str(prop_emp)\n",
      "                print 'emp target is ' + str(target_emp)\n",
      "                varname = 'zgpgroup%s' % (cid)\n",
      "                print varname\n",
      "                if (prop_ru > (target_ru - .01)) and (prop_ru < (target_ru + .01)):\n",
      "                    print 'NO ru action.'\n",
      "                    i = i + 1\n",
      "                elif math.isnan(prop_ru) or (prop_ru < target_ru):\n",
      "                    for submodel in ru_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] + delta\n",
      "                    print 'ru action is PLUS'\n",
      "                elif prop_ru > target_ru:\n",
      "                    for submodel in ru_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] - delta\n",
      "                    print 'ru action is MINUS'\n",
      "                    \n",
      "                if (prop_hh > (target_hh - .01)) and (prop_hh < (target_hh + .01)):\n",
      "                    print 'NO hh action.'\n",
      "                    j = j + 1\n",
      "                elif math.isnan(prop_hh) or (prop_hh < target_hh):\n",
      "                    for submodel in hh_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] + delta\n",
      "                    print 'hh action is PLUS'\n",
      "                elif prop_hh > target_hh:\n",
      "                    for submodel in hh_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] - delta\n",
      "                    print 'hh action is MINUS'\n",
      "                    \n",
      "                if (prop_emp > (target_emp - .01)) and (prop_emp < (target_emp + .01)):\n",
      "                    print 'NO emp action.'\n",
      "                    k = k + 1\n",
      "                elif math.isnan(prop_emp) or (prop_emp < target_emp):\n",
      "                    for submodel in emp_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] + delta\n",
      "                    print 'emp action is PLUS'\n",
      "                elif prop_emp > target_emp:\n",
      "                    for submodel in emp_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] - delta\n",
      "                    print 'emp action is MINUS'\n",
      "                    \n",
      "                if (prop_nonres > (target_nonres - .01)) and (prop_nonres < (target_nonres + .01)):\n",
      "                    print 'NO nonres action.'\n",
      "                    m = m + 1\n",
      "                elif math.isnan(prop_nonres) or (prop_nonres < target_nonres):\n",
      "                    for submodel in nr_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] + delta\n",
      "                    print 'nonres action is PLUS'\n",
      "                elif prop_nonres > target_nonres:\n",
      "                    for submodel in nr_submodels:\n",
      "                        dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] = dset.coeffs[(submodel, 'coeffs')][dset.coeffs[(submodel,'fnames')]==varname] - delta\n",
      "                    print 'nonres action is MINUS'\n",
      "            print i,j,k,m\n",
      "            ###Save calibrated coefficients at the end of each iteration\n",
      "            coeff_store_path = os.path.join(output_dir,'coeffs.h5')\n",
      "            coeff_store = pd.HDFStore(coeff_store_path)\n",
      "            coeff_store['coeffs'] = dset.coeffs\n",
      "            coeff_store.close()\n",
      "#             if (i == 12) and (j==12) and (k==12) and (m==12):\n",
      "#                 break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Calibration iteration 0\n",
        "Simulating year 2000\n",
        "Fetching buildings\n",
        "Fetching modify_table\n",
        "Fetching households\n",
        "Fetching modify_table"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fetching establishments"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elapsed = time.time() - seconds_start\n",
      "print \"TOTAL elapsed time: \" + str(elapsed) + \" seconds.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}